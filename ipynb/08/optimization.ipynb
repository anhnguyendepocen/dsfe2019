{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In [The Mean and Slopes](mean_and_slopes), we used a simple but slow way to\n",
    "find the slope that best predicted one vector of values from another vector of\n",
    "values.\n",
    "\n",
    "First we go back to find that slope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# Make plots look a little bit more fancy\n",
    "plt.style.use('fivethirtyeight')\n",
    "# Print to 2 decimal places, show tiny values as 0\n",
    "np.set_printoptions(precision=2, suppress=True)\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the [gender_stats.csv](https://matthew-brett.github.io/cfd2019/data/gender_stats.csv) file\n",
    "to the same directory as this notebook, if you are running on your own computer.\n",
    "\n",
    "Fetch and process the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data file\n",
    "gender_data = pd.read_csv('gender_stats.csv')\n",
    "# Make new data frame with only MMR and FR\n",
    "just_mmr_fr = gender_data[['mat_mort_ratio', 'fert_rate']]\n",
    "# Drop the NaN values, and make into arrays.\n",
    "clean_mmr_fr = just_mmr_fr.dropna()\n",
    "mmr = np.array(clean_mmr_fr['mat_mort_ratio'])\n",
    "fert = np.array(clean_mmr_fr['fert_rate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our criterion is the sum of squared error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sos_error(slope):\n",
    "    fitted = fert * slope  # 'fert' comes from the top level\n",
    "    error = mmr - fitted     # 'mmr' comes from the top level\n",
    "    return np.sum(error ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found the best slope by trying a very large number of slopes, and recording, for each slope, the sum of squared error.  We chose the slope from the slopes that we tried, that gave us the lowest sum of squared error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slopes to try\n",
    "some_slopes = np.arange(50, 110, 0.1)\n",
    "n_slopes = len(some_slopes)\n",
    "# Try all these slopes, calculate and record sum of squared error\n",
    "sos_errors = np.zeros(n_slopes)\n",
    "for i in np.arange(n_slopes):\n",
    "    slope = some_slopes[i]\n",
    "    sos_errors[i] = sos_error(slope)\n",
    "# The slope minimizing the sum of squared error\n",
    "best_slope = some_slopes[np.argmin(sos_errors)]\n",
    "best_slope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end, of [the mean and slopes](mean_and_slopes) notebook, we saw that\n",
    "a function in Scipy called `minimize` can do this work for us, relatively\n",
    "quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "minimize(sos_error, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we are doing, with our slow dumb technique, and with the `minimize`\n",
    "function, is something called [mathematical\n",
    "optimization](https://en.wikipedia.org/wiki/Mathematical_optimization).  We\n",
    "use *optimization* when we have some *function* that takes one or more\n",
    "*parameters*.  We want to chose, or *optimize* the parameters to give us some\n",
    "desired output from the function. \n",
    "\n",
    "In our case our *function* is the sum of squared error, `sos_error`.  The *parameter* is the slope.  We are trying to find the value for the parameter that *minimizes* the result of calling the function `sos_error`.\n",
    "\n",
    "One way of doing this minimization, is the slow dumb way.  We just try a huge number of values for the parameter (the slope), and chose the value that gives us the lowest output value (the sum of squared error).\n",
    "\n",
    "This is such a common problem, that there has been an enormous amount of theoretical and practical work on building algorithms to make process of searching for the minimum value more efficient.\n",
    "\n",
    "This notebook is to give you an idea of how you might do this, and therefore, the kind of things that `minimize` can do, to search quickly for the best parameter.\n",
    "\n",
    "Let's look again at the shape of the curve relating the slope to the sum of squared error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(some_slopes, sos_errors)\n",
    "plt.xlabel('Candidate slopes')\n",
    "plt.ylabel('Sum of squared error')\n",
    "plt.title('SSE as a function of slope')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the *function* we are trying minimize.  Specifically, we are trying to\n",
    "optimize the function that gives the *SSE* as a function of the *slope*\n",
    "parameter.\n",
    "\n",
    "We want to avoid trying every possible value for the slope.\n",
    "\n",
    "So, we are going to start with one value for the slope, say 100, then see if there is a good way to chose the next value to try.\n",
    "\n",
    "Looking at the graph, we see that, when the slope is far away from the minimum, the sum of squared error (on the y axis) changes very quickly as the slope changes.  That is, the function has a steep *gradient*.\n",
    "\n",
    "Maybe we could check what the gradient is, at our starting value of 100, by calculating the sum of squares (y) value, and then calculating the sum of squares (y) value when we increase the slope by a tiny amount.  This is the change in y for a very small change in x.  We divide the change in y by the change in x, to get the gradient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sos_error_gradient(x, dx=0.0001):\n",
    "    # Gradient of the sos_error at this value of x\n",
    "    # sos_error at this x value.\n",
    "    sos_0 = sos_error(x)\n",
    "    # sos_error a tiny bit to the right on the x axis.\n",
    "    sos_1 = sos_error(x + dx)\n",
    "    # gradient is y difference divided by x difference.\n",
    "    return (sos_1 - sos_0) / dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The y value of the function.\n",
    "sos_error(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The gradient of the function at this point.\n",
    "sos_error_gradient(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A large positive gradient means the x value (slope) that we tried is still far to the *right* of the minimum. This might encourage us to try an x value that is well to the left.  We could call this a large step in x, and therefore a large *step size*.\n",
    "\n",
    "A large negative gradient means the x value (slope) that we tried is still far to the *left* of the minimum.   This might encourage us to try an x value that is well to the right.\n",
    "\n",
    "As the gradients get small, we want to take smaller steps, so we don't miss the minimum.\n",
    "\n",
    "The general idea then, is to chose our step sizes in proportion to the gradient of the function.\n",
    "\n",
    "This is the optimization technique known as [gradient descent](https://en.wikipedia.org/wiki/Gradient_descent).\n",
    "\n",
    "Here it is in action, using code modified from the Wikipedia page above.\n",
    "\n",
    "We try new x (slope) values by making big jumps when the gradient is steep, and small jumps when the gradient is shallow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_x = 100 # We start the search at x=100\n",
    "gamma = 0.0001 # Step size multiplier\n",
    "precision = 0.00001 # Desired precision of result\n",
    "max_iters = 1000 # Maximum number of iterations\n",
    "\n",
    "for i in np.arange(max_iters):\n",
    "    # Go to the next x value\n",
    "    current_x = next_x\n",
    "    # Estimate the gradient\n",
    "    gradient = sos_error_gradient(current_x)\n",
    "    # Use gradient to choose the next x value to try.\n",
    "    # This takes negative steps when the gradient is positive\n",
    "    # and positive steps when the gradient is negative.\n",
    "    next_x = current_x - gamma * gradient\n",
    "    step = next_x - current_x\n",
    "    print('x: {:0.5f}; step {:0.5f}; gradient {:0.2f}'.format(\n",
    "        current_x, step, gradient))\n",
    "    # When the step size is equal to or smaller than the desired\n",
    "    # precision, we are near enough.\n",
    "    if abs(step) <= precision:\n",
    "        # Break out of the loop.\n",
    "        break\n",
    "\n",
    "print(\"Minimum at\", next_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, by doing this, we save ourselves from trying a very large number of other potential x values (slopes), and we focus in on the minimum very quickly.\n",
    "\n",
    "This is just one method among many for optimizing our search for the minimum\n",
    "of a function.  Now you know what kind of thing it is doing, we will just let\n",
    "`miminize` do its job for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use float to show us the final result in higher precision\n",
    "result = minimize(sos_error, 100)\n",
    "float(result.x)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "metadata_filter": {
    "notebook": {
     "additional": "all",
     "excluded": [
      "language_info"
     ]
    }
   },
   "text_representation": {
    "extension": ".Rmd",
    "format_name": "rmarkdown",
    "format_version": "1.0",
    "jupytext_version": "0.8.6"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
