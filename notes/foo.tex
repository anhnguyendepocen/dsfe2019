\hypertarget{notes-from-the-berkeley-textbook}{%
\section{Notes from the Berkeley
textbook}\label{notes-from-the-berkeley-textbook}}

Textbook is \href{https://www.inferentialthinking.com}{Inferential and
Computational Thinking}

\hypertarget{content}{%
\subsection{Content}\label{content}}

Here's the summary, with annotations

\begin{itemize}
\tightlist
\item
  \href{chapters/01/what-is-data-science.md}{Data Science}

  \begin{itemize}
  \tightlist
  \item
    \href{chapters/01/1/intro.md}{Introduction}

    \begin{itemize}
    \tightlist
    \item
      \href{chapters/01/1/1/computational-tools.md}{Computational Tools}
    \item
      \href{chapters/01/1/2/statistical-techniques.md}{Statistical
      Techniques}
    \end{itemize}
  \item
    \href{chapters/01/2/why-data-science.md}{Why Data Science?}
  \item
    \href{chapters/01/3/plotting-the-classics.md}{Plotting the Classics}

    \begin{itemize}
    \tightlist
    \item
      \href{chapters/01/3/1/literary-characters.md}{Literary Characters}
    \item
      \href{chapters/01/3/2/another-kind-of-character.md}{Another Kind
      of Character}
    \end{itemize}
  \end{itemize}
\item
  \href{chapters/02/causality-and-experiments.md}{Causality and
  Experiments}

  \begin{itemize}
  \tightlist
  \item
    \href{chapters/02/1/observation-and-visualization-john-snow-and-the-broad-street-pump.md}{John
    Snow and the Broad Street Pump}
  \item
    \href{chapters/02/2/snow-s-grand-experiment.md}{Snow's ``Grand
    Experiment''}
  \item
    \href{chapters/02/3/establishing-causality.md}{Establishing
    Causality} Is it just an association? The idea of a control group.
    Comparing groups that only differ in the attribute of interest.
  \item
    \href{chapters/02/4/randomization.md}{Randomization} Randomization
    as a way to make a good control group. Randomized controlled trials.
    Blind trials.
  \item
    \href{chapters/02/5/endnote.md}{Endnote}
  \end{itemize}
\item
  \href{chapters/03/programming-in-python.md}{Programming in Python}

  \begin{itemize}
  \tightlist
  \item
    \href{chapters/03/1/expressions.md}{Expressions}
  \item
    \href{chapters/03/2/numbers.md}{Numbers}
  \item
    \href{chapters/03/3/names.md}{Names}

    \begin{itemize}
    \tightlist
    \item
      \href{chapters/03/3/1/example-growth-rates.md}{Example: Growth
      Rates}
    \end{itemize}
  \item
    \href{chapters/03/4/call-expressions.md}{Call Expressions} Calling
    functions. round, max. Different numbers of arguments.
  \end{itemize}
\item
  \href{chapters/04/data-types.md}{Data Types}

  \begin{itemize}
  \tightlist
  \item
    \href{chapters/04/1/strings.md}{Strings} Adding strings. Converting
    numbers to strings with \texttt{str}

    \begin{itemize}
    \tightlist
    \item
      \href{chapters/04/1/1/string-methods.md}{String Methods} The idea
      of a function attached to an object. \texttt{upper},
      \texttt{replace}.
    \end{itemize}
  \item
    \href{chapters/04/2/comparisons.md}{Comparisons} Including boolean
    variables. Comparison of strings.
  \item
    \href{chapters/04/3/sequences.md}{Sequences} Straight to arrays,
    with \texttt{make\_array}. \texttt{sum}.
  \item
    \href{chapters/04/4/arrays.md}{Arrays} String and numeric arrays.
    Elementwise * and + with scalars. \texttt{size}, \texttt{sum},
    \texttt{mean} methods. \texttt{import\ numpy\ as\ np}.
  \item
    \href{chapters/04/5/ranges.md}{Ranges} In fact, \texttt{arange}.
  \item
    \href{chapters/04/6/more-on-arrays.md}{More on Arrays} Elementwise
    combination of arrays.
  \end{itemize}
\item
  \href{chapters/05/tables.md}{Tables} Creating tables from data.
  Loading Minard data table. Adding columns to a table. Number of
  columns. Number of rows. Column names. Renaming columns. Getting data
  from columns. Formatting print output of columns. Selecting columns.
  Dropping columns.

  \begin{itemize}
  \tightlist
  \item
    \href{chapters/05/1/sorting-rows.md}{Sorting Rows} \texttt{show} to
    show some rows. \texttt{sort} by a column. \texttt{descending} as a
    option, and keyword arguments.
  \item
    \href{chapters/05/2/selecting-rows.md}{Selecting Rows} By index,
    integer(s). By features -
    \texttt{t.where(\textquotesingle{}SALARY\textquotesingle{}),\ are.above(10))}.
  \item
    \href{chapters/05/3/example-trends-in-the-population-of-the-united-states.md}{Example:
    Population Trends} Subtracting columns.
  \item
    \href{chapters/05/4/example-gender-ratio-in-the-us-population.md}{Example:
    Trends in Gender} Increasing F:M ratio as a function of age.
  \end{itemize}
\item
  \href{chapters/06/visualization.md}{Visualization} IMDB etc databases.
  Scatter plots, line graphs.

  \begin{itemize}
  \tightlist
  \item
    \href{chapters/06/1/visualizing-categorical-distributions.md}{Categorical
    Distributions} Ice cream example. Bar charts, particularly
    horizontal bar charts. Sorting categories.
  \item
    \href{chapters/06/2/visualizing-numerical-distributions.md}{Numerical
    Distributions} Histograms. Bins. Unequal bins. Counts vs
    proportions. Differences between bar charts and histograms.
    Grouping. Numerical variables as categories, and confusion
    therefrom.
  \item
    \href{chapters/06/3/overlaid-graphs.md}{Overlaid Graphs} Categories
    on scatter plots, histograms, bar charts, line graphs.
  \end{itemize}
\item
  \href{chapters/07/functions-and-tables.md}{Functions and Tables}
  Defining functions. Local scope. Docstrings. Multiple arguments.

  \begin{itemize}
  \tightlist
  \item
    \href{chapters/07/1/applying-a-function-to-a-column.md}{Applying
    Functions to Columns} Functions as values. Passing a function.
    Applying a function to a row of data.
  \item
    \href{chapters/07/2/classifying-by-one-variable.md}{Classifying by
    One Variable} Group, then count. Group and sum, arbitrary functions.
  \item
    \href{chapters/07/3/cross-classifying-by-more-than-one-variable.md}{Cross-Classifying}
    Classifying by more than one variable. \texttt{group} again, and
    \texttt{pivot}.
  \item
    \href{chapters/07/4/joining-tables-by-columns.md}{Joining Tables by
    Columns} The \texttt{join} method.
  \item
    \href{chapters/07/5/bike-sharing-in-the-bay-area.md}{Bike Sharing in
    the Bay Area} Using various table methods. \texttt{datascience}
    classes \texttt{Marker.map\_table}, \texttt{Circle.map\_table}.
  \end{itemize}
\item
  \href{chapters/08/randomness.md}{Randomness}
  \texttt{np.random.choice}; comparisons, and booleans. Comparing
  strings. Comparisons in arrays. \texttt{count\_nonzero}.

  \begin{itemize}
  \tightlist
  \item
    \href{chapters/08/1/conditional-statements.md}{Conditional
    Statements} \texttt{if}, \texttt{elif}, \texttt{else}, demonstrated
    inside functions.
  \item
    \href{chapters/08/2/iteration.md}{Iteration} \texttt{for}, using
    \texttt{np.arange}. Unrolling loops. \texttt{np.append}.
  \item
    \href{notebooks/09/3/Simulation.ipynb}{Simulation} The idea of
    simulation. How to simulate. Heads and tails. Histograms, for loops.
    Rolling two dice in monopoly.
  \item
    \href{chapters/08/3/monty-hall-problem.md}{The Monty Hall Problem}
    Simulations with functions and arrays.
  \item
    \href{chapters/08/4/finding-probabilities.md}{Finding Probabilities}
    Probabilities of combined events - multiplying. Conditional
    probabilities. Probabilities when there are two different ways of
    something happening.
  \end{itemize}
\item
  \href{notebooks/10/Sampling_and_Empirical_Distributions.ipynb}{Sampling
  and Empirical Distributions} Random and not-random samples. Sampling
  with uneven but known probability. Sampling every N, after starting at
  a random point (\emph{systematic} sample). Sampling with and without
  replacement.

  \begin{itemize}
  \tightlist
  \item
    \href{notebooks/10/1/Empirical_Distributions.ipynb}{10.1 Empirical
    Distributions} Theoretical distributions. Observed distributions.
    Law of averages - as the sample number increases, the empirical
    distribution becomes more like the theoretical.
  \item
    \href{notebooks/10/2/Sampling_from_a_Population.ipynb}{10.2 Sampling
    from a Population} Distribution of a random samples from population
    becomes more like distribution of population, as sample size
    increases.
  \item
    \href{notebooks/10/3/Empirical_Distribution_of_a_Statistic.ipynb}{10.3
    Empirical Distibution of a Statistic} Numerical quantities from a
    population = \emph{parameters}. Numerical quantities from samples:
    \emph{statistics}. Statistic != Parameter. How different could it
    have been?
  \end{itemize}
\item
  \href{notebooks/11/Testing_Hypotheses.ipynb}{11. Testing Hypotheses}
  Answers for yes/no questions from data.

  \begin{itemize}
  \tightlist
  \item
    \href{notebooks/11/1/Assessing_Models.ipynb}{11.1 Assessing Models}
    Model == ``a set of assumptions about data''. Statistic. Predicting
    statistic under the model. Compare prediction and data. Swain's
    jury. Mendel's flowers.
  \item
    \href{notebooks/11/2/Multiple_Categories.ipynb}{11.2 Multiple
    Categories} Race from California jury selections. Total variation
    distance as measure of distance between distributions. Simulating
    jurors, and variation distances. Explanations for failure of model.
  \item
    \href{notebooks/11/3/Decisions_and_Uncertainty.ipynb}{11.3 Decisions
    and Uncertainty} Null-hypothesis, alternative hypothesis, test
    statistic, distribution under the null, conclusion. How to decide
    whether statistic is too unusual. Ronald Fisher and 0.05.
  \end{itemize}
\item
  \href{notebooks/12/Comparing_Two_Samples.ipynb}{12. Comparing Two
  Samples} Instead of comparing sample to known population.

  \begin{itemize}
  \tightlist
  \item
    \href{notebooks/12/1/AB_Testing.ipynb}{12.1 A/B Testing} Mean
    difference of birth weights to smokers / non-smokers. Permutation.
  \item
    \href{notebooks/12/2/Deflategate.ipynb}{12.2 Deflategate}
    Permutation test for Deflategate pressure measurements.
  \item
    \href{notebooks/12/3/Causality.ipynb}{12.3 Causality} Proportion of
    patients with pain relief from Botulinum toxin. Random assignment of
    patients. Permutation test of proportions. Difficulty of drawing
    firm conclusions.
  \end{itemize}
\item
  \href{notebooks/13/Estimation.ipynb}{13. Estimation} Sample statistic
  to estimate parameter. ``How different could this estimate have been,
  if the sample had come out differently?''

  \begin{itemize}
  \tightlist
  \item
    \href{notebooks/13/1/Percentiles.ipynb}{13.1 Percentiles}
    Calculating percentiles. Quartiles.
  \item
    \href{notebooks/13/2/Bootstrap.ipynb}{13.2 The Bootstrap} Bootstrap
    for estimating variability of statistic. Salaries in SF. Estimate of
    median from sample using bootstrap. Do estimates \emph{capture} the
    parameter.
  \item
    \href{notebooks/13/3/Confidence_Intervals.ipynb}{13.3 Confidence
    Intervals} Bootstrap for 95\% (etc) confidence intervals, mean baby
    birth weight. Bootstrap for proportions. Caveats for bootstrap.
  \item
    \href{notebooks/13/4/Using_Confidence_Intervals.ipynb}{13.4 Using
    Confidence Intervals} Testing whether a parameter is plausible.
    Hodgkin's Lymphoma, comparing drop in lung function over time, after
    treatment. Comparing drop to 0, using confidence intervals.
  \end{itemize}
\item
  \href{notebooks/14/Why_the_Mean_Matters.ipynb}{14. Why the Mean
  Matters} Empirical distribution of mean is generally normal.

  \begin{itemize}
  \tightlist
  \item
    \href{notebooks/14/1/Properties_of_the_Mean.ipynb}{14.1 Properties
    of the Mean} Mean as ``smoother''. Depends only on distribution of
    values (not numbers). Balance point of histogram. Relationship of
    median and mean.
  \item
    \href{notebooks/14/2/Variability.ipynb}{14.2 Variability} Deviations
    from average. Squared deviations. Mean squared deviation == variance
    -\textgreater{} SD. Most observations are a few standard SDs from
    the mean. Chebychev.
  \item
    \href{notebooks/14/3/SD_and_the_Normal_Curve.ipynb}{14.3 The SD and
    the Normal Curve} Point of inflection. Standard units, and the
    standard normal curve. The CDF. Proportion of area between +/-1 and
    2 SDs.
  \item
    \href{notebooks/14/4/Central_Limit_Theorem.ipynb}{14.4 The Central
    Limit Theorem} Simulation of sum of red / black choice in red
    -\textgreater{} normal. Average flight delay (where flight delay is
    positive-skewed). Proportion of purple flowers in Mendel's data.
    Width of sampling distribution as a function of sample size. No
    relationship to population size. Large samples make result valid for
    any distribution.
  \item
    \href{notebooks/14/5/Variability_of_the_Sample_Mean.ipynb}{14.5 The
    Variability of the Sample Mean} Width of sampling distribution,
    sample size. Root N as scaling for SD.
  \item
    \href{notebooks/14/6/Choosing_a_Sample_Size.ipynb}{14.6 Choosing a
    Sample Size} Using SDs to predict confidence intervals, and
    therefore, required sample size for given levels of confidence.
  \end{itemize}
\item
  \href{notebooks/15/Prediction.ipynb}{15. Prediction} Predicting child
  height (y) from parents' height (x), by averaging over x intervals.

  \begin{itemize}
  \tightlist
  \item
    \href{notebooks/15/1/Correlation.ipynb}{15.1 Correlation} MPG /
    acceleration data. Changing to standard units. Correlation and
    scatter, in standard units. ``r measures the extent to which the
    scatter plot clusters around a straight line''. r and the dot
    product of x, y in standard units. Not causation. Only measures
    linear association.
  \item
    \href{notebooks/15/2/Regression_Line.ipynb}{15.2 The Regression
    Line} The regression line and the 45 degree line. Regression slope
    from correlation coefficient. Prediction from the regression line.
    Meaning of the regression slope.
  \item
    \href{notebooks/15/3/Method_of_Least_Squares.ipynb}{15.3 The Method
    of Least Squares} ``Best'' line. Line, predictions, and prediction
    error. Root mean squared error. Trying different lines. Regression
    line as best in terms of RMSE. Finding the line by numerical
    optimization.
  \item
    \href{notebooks/15/4/Least_Squares_Regression.ipynb}{15.4 Least
    Squares Regression} Shot-put weight lift / shot put distance
    correlation. Least squares line still least squares line, even if
    plot is not rugby-ball shaped. Find quadratic line using
    \texttt{minimize}.
  \item
    \href{notebooks/15/5/Visual_Diagnostics.ipynb}{15.5 Visual
    Diagnostics} Plotting residuals against parents' heights. Good
    regression -\textgreater{} ``no pattern'' the residuals. Residual
    plot of whale length and age reveals non-linearity in data.
    Heteroscedasticity (``uneven spread''). Acceleration (x) vs MPG (y).
    Residuals more variable for lower acceleration. Regression estimates
    less accurate for lower acceleration values.
  \item
    \href{notebooks/15/6/Numerical_Diagnostics.ipynb}{15.6 Numerical
    Diagnostics} x vs residual plot has slope (very near) 0. Average of
    residuals always 0. SD of residuals predictable from SD of y and r.
  \end{itemize}
\item
  \href{notebooks/16/Inference_for_Regression.ipynb}{16. Inference for
  Regression} From regression line in sample to inference on the
  population.

  \begin{itemize}
  \tightlist
  \item
    \href{notebooks/16/1/Regression_Model.ipynb}{16.1 A Regression
    Model} Regression model as true line plus random noise. Generating
    some points from the true line. The regression line as estimate.\\
  \item
    \href{notebooks/16/2/Inference_for_the_True_Slope.ipynb}{16.2
    Inference for the True Slope} Bootstrapping the regression line.
    95\% confidence interval. Non-zero regression slope when true line
    has slope 0. Inference on the slope with null of 0.
  \item
    \href{notebooks/16/3/Prediction_Intervals.ipynb}{16.3 Prediction
    Intervals} Bootstrap regression to get prediction intervals for all
    points on line.
  \end{itemize}
\item
  \href{notebooks/17/Classification.ipynb}{17. Classification} Examples
  of classifiers (fraudulent orders, compatible matches for dating
  websites, diagnosis of cancer, predicting votes. Observations.
  Attributes. Class (fraudulent, not-fraudulent). Training and testing
  sets. Can be imperfect.

  \begin{itemize}
  \tightlist
  \item
    \href{notebooks/17/1/Nearest_Neighbors.ipynb}{17.1 Nearest
    Neighbors} Kidney disease. Hemoglobin, glucose. Classify new point.
    Nearest neighbor. Decision boundary. More difficult classifier - WBC
    and glucose. k nearest neighbors.
  \item
    \href{notebooks/17/2/Training_and_Testing.ipynb}{17.2 Training and
    Testing} Testing against unknown points - the testing set. Problem
    of testing nearest neighbor classifier on training set. Split data
    in half.
  \item
    \href{notebooks/17/3/Rows_of_Tables.ipynb}{17.3 Rows of Tables} Rows
    as attributes of observations. Making rows into arrays. Pythagoras
    and Euclidean distance. Distance function. Applying functions to
    rows in table with \texttt{apply}. Apply ``distance to new point''
    function. Select top five rows for this distance. Take majority vote
    on classification.
  \item
    \href{notebooks/17/4/Implementing_the_Classifier.ipynb}{17.4
    Implementing the Classifier} Classifying banknotes. Complex patterns
    of class on scatterplot. Three attributes and Pythagoras. More than
    three. Distance function for N-D. Wine dataset, classifying grape
    species. General k-nearest neighbors classifier.
  \item
    \href{notebooks/17/5/Accuracy_of_the_Classifier.ipynb}{17.5 The
    Accuracy of the Classifier} Hold-out method (splitting into test and
    training). 95\% prediction success for test set. Breast cancer,
    school competition. kNN classifier. 96\% accuracy.
  \item
    \href{notebooks/17/6/Multiple_Regression.ipynb}{17.6 Multiple
    Regression} House prices in Iowa. Prediction with slopes for each
    predicting variable. RMSE as criterion. Use \texttt{minimize} to
    find best slopes for training set. Residual plot - underestimation
    of high priced houses. kNN prediction (average of price of the
    kNNs).
  \end{itemize}
\item
  \href{notebooks/18/Updating_Predictions.ipynb}{18. Updating
  Predictions}

  \begin{itemize}
  \tightlist
  \item
    \href{notebooks/18/1/More_Likely_than_Not_Binary_Classifier.ipynb}{18.1
    A ``More Likely Than Not'' Binary Classifier} Bayes rule, students
    in second and third years, majors declared or not. The tree diagram.
  \item
    \href{notebooks/18/2/Making_Decisions.ipynb}{18.2 Making Decisions}
    False positives, false negatives. Test for rare disease with low
    false positive rate. Table expressing known probabilities to
    demonstrate Bayes rule. Effect of priors (subjective probability).
  \end{itemize}
\end{itemize}

\hypertarget{homework}{%
\subsection{Homework}\label{homework}}

For example, see
\url{https://github.com/data-8/data8assets/blob/gh-pages/materials/su17}.

\hypertarget{datasets}{%
\subsection{Datasets}\label{datasets}}

\begin{itemize}
\tightlist
\item
  \href{http://users.stat.ufl.edu/~winner/datasets.html}{Larry Winner's
  dataset list}.
\item
  \url{https://medium.com/datadriveninvestor/the-50-best-public-datasets-for-machine-learning-d80e9f030279}.
\end{itemize}
